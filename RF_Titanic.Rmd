---
title: "Titanic Data Sets and RandomForests"
author: "Trey Beeman"
date: "December 9, 2016"
output: 
  html_document:
    number sections: true
    toc: true
    theme: readable
    highlight: tango
---
# __1 Introduction__

### __1.1 Abstract__

This document is being created to learn `randomForests` working with data from Kaggle competitions.  Thanks to Kaggle.com and Datacamp.com for the information and guidance in preparing this document.  I relied heavily on this source (https://www.kaggle.io/svf/198371/166ea2e9c1074ca9cd2447c7ee27cf10/__results__.html#prediction) in order to work through the feature engineering techniques.

### __1.2 Libraries__
This is an exhaustive list of all libraries used for this project.
```{r libraries, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(randomForest)
library(rpart)
library(caret)
library(rattle)
library(rpart.plot)
library(mice)
library(RColorBrewer)
```

# __2 Data__
We read the data sets into R.  The data can be found at the links in the code block. 
```{r load datasets}
# create url strings
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"

# read the data in to R.
train <- read.csv(train_url, stringsAsFactors = FALSE)
test <- read.csv(test_url, stringsAsFactors = FALSE)

# combine the data into a single dataframe for feature engineering
df <- bind_rows(train,test)

```

### __2.1 Exploratory Data Analysis__ 
We take a look at the data to gain insight into what can be pulled out of it.

A `str()` of the data with an overview of the data, class and variabe names. 
```{r, echo=FALSE}
str(df)
```

We will highlight a few useful or interesting facts from the data.

This is a proportion table of the survivors by `Sex`.
```{r, echo=FALSE}
prop.table(table(df$Sex, df$Survived),1)
```

This is a proportion table of survivors by Passenger Class `Pclass`.
```{r, echo=FALSE}
prop.table(table(df$Pclass, df$Survived),1)
```

##### __2.1.1 The Passengers__
The mean age of all passengers is: 

```{r, echo = FALSE}
mean(df$Age,na.rm = TRUE)
```

The mininum age of all passengers is:
```{r, echo = FALSE}
min(df$Age,na.rm = TRUE)
```

The maxinum age of all passengers is:
```{r, echo = FALSE}
max(df$Age,na.rm = TRUE)
```

##### __2.2.2 The Survivors__
The mean age of survivors is:
```{r, echo = FALSE}
mean(df$Age[df$Survived == 1],na.rm = TRUE)
```

The mininum age of survivors:
```{r, echo = FALSE}
min(df$Age[df$Survived == 1],na.rm = TRUE)
```

The maximum age of survivors:
```{r, echo = FALSE}
max(df$Age[df$Survived == 1],na.rm = TRUE)
```

### __2.2 Exploratory Graphs__
These graphs are intended to explore the  
```{r exploratory graphs, echo = FALSE}
hist(df$Age, breaks = seq(0, 80, 5), main = "Histogram of Ages of All Passengers", col = "lightblue", xlab = "Passenger Ages in groups of 5 years")
hist(df$Age[train$Survived == 1], breaks = seq(0, 80, 5), main = "Histogram of Ages of Surviving Passengers", col = "lightblue", xlab = "Survivors Ages in groups of 5 years")
```

# __3 Feature Engineering__

### __3.1 Names__
We will create or __feature engineer__ a new variable based first on names.  We create a `Title` variable.
```{r titles}
# pull the title out of the passenger names
df$Title <- gsub('(.*, )|(\\..*)', '', df$Name)

table(df$Sex, df$Title)
```

First, we will quickly clean up the titles with are synonyms and congnates.
```{r titles cognates}
df$Title[df$Title == "Ms"] <- "Miss"
df$Title[df$Title == "Mlle"] <- 'Miss'
df$Title[df$Title == "Mme"] <- 'Mrs'
```

With the remaining, we will divide them into classes of __nobility, military, Dr__ and __Rev__ in order to preserve the information in these titles.

```{r titles honorific}
nobility <- c("Don", "Dona", "Jonkheer", "Lady", "Sir", "the Countess")
military <- c("Capt", "Col", "Major")

df$Title[df$Title %in% nobility] <- "Nobility"
df$Title[df$Title %in% military] <- "Military"
```

It is interesting to note that all the individuals with the title __Rev__ did not survive.
```{r titles graph, echo=FALSE}
ggplot(data = df[(df$Title %in% c("Dr", "Military", "Nobility", "Rev")) & (!is.na(df$Survived)),], aes(x = Title, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + labs(x = 'Titles')
```

### __3.2 Families__ 

We want to examine the hypothesis that families are likely to move together as a group, therefore, their survivorship may differ from an individual travelling alone.  We will create `family_size` variable to indicate a family travelling together.

```{r families 1}
# create a new variable, family_size
df$family_size <- df$SibSp + df$Parch + 1

# create Surname variable 
df$Surname <- sapply(df$Name, function(x) strsplit(x, split = '[,.]')[[1]][1])

# Create a family variable identifying a group travelling together
df$Family <- paste(df$Surname, df$family_size, sep='_')
```

We can see that individuals and familes greater than 4 had a lower survivorship than other groups. 
```{r families graph 1}
ggplot(data = subset(df, !is.na(df$Survived)), aes(x = family_size, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + scale_x_continuous(breaks=c(1:11)) + labs(x = 'Family Size')
```

We are going to created a __discretized__ variable on family size in order to examine this data a little more closely.

```{r discetize family var}
# create the vatiables
df$Disfamily_size[df$family_size == 1] <- 'singleton'
df$Disfamily_size[df$family_size < 5 & df$family_size > 1] <- 'small'
df$Disfamily_size[df$family_size > 4] <- 'large'

# mosaicplot of the 
mosaicplot(table(df$Disfamily_size, df$Survived), main='Family Size by Survival', shade=TRUE)
```

The mosaic plot confirms our hyopthesis that there is a penalty for individuals and large families.

### __3.3 Cabin__
We will create a variable for the Deck based on the information about the cabin assignments.

```{r create deck variable}
df$Deck <- factor(sapply(df$Cabin, function(x)  strsplit(x, "")[[1]][1]))
```


# __4 Missing Values__

### __4.1 Simple Value Imputation__
In order to continue we need to deal with missing values.  First, we look at where they are missing values.

```{r where are missing values}
# search for blanks
sapply(df, function(x) sum(grepl("^$", x)))

# search for NAs
sapply(df, function(x) sum(is.na(x)))
```

We start small. __Embarked__ is missing two values.
```{r embarked missing values}
# which values are missing
which(grepl("^$", df$Embarked) == TRUE)

# examine the data 
df[62,]
df[830,]
```

Looking at the data there is an interesting coincidence, these ladies shared a cabin.  In this case, given the small number of changes, an investigation into the names revealed that there is documentation of these ladies. Ms. Icard was Mrs. Stone's maid and they boarded Titanic at Southhampton per this website (https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html)

In this instance, we will assign the values based on this information.
```{r embarked assign values}
df$Embarked[62] <- df$Embarked[830] <- "S"
```

__Fare__ is missing one value.
```{r fare missing value, warning=FALSE}
# examine the data
df[which(is.na(df$Fare) == TRUE),]

ggplot(df[df$Pclass == '3' & df$Embarked == 'S', ], aes(x = Fare)) +
   geom_density(fill = '#99d6ff', alpha=0.4) + 
   geom_vline(aes(xintercept=median(Fare, na.rm=TRUE)),
              colour='red', linetype='dashed', lwd=1)
```

Mr. Storey departed from Southhamptom and travelled 3rd class, so we will use those variables to base an imputation.  It appears reasonable to assume the median for his __Pclass__ and __Embarked__ values.

```{r fare imputed value}
df$Fare[1044] <- median(df[df$Pclass == "3" & df$Embarked == "S",]$Fare, na.rm = TRUE)
```

### __4.2 Age__
We tackle __Age__ next which has 263 missing values.  We will use predicitve imputation via the `mice` package.

```{r age}
# coerce variables to factors
factor_vars <- c("PassengerId", "Pclass", "Sex", "Embarked", "Title", "Surname", "Family", "Disfamily_size")

df[factor_vars] <- lapply(df[factor_vars], function(x) as.factor(x))

set.seed(129)

mice_mod <- mice(df[, !names(df) %in% c('PassengerId','Name','Ticket','Cabin','Family','Surname','Survived')], method="rf")

mice_output <- complete(mice_mod)
```

Now, we compare the distruibution of the imputed values with the observed values.

```{r mice chack graph}
# hist of age distributions
par(mfrow=c(1,2))
hist(df$Age, freq=FALSE, main='Age: Original Data', 
  col='darkblue', ylim=c(0,0.04))
hist(mice_output$Age, freq=FALSE, main='Age: MICE Output', 
  col='lightblue', ylim=c(0,0.04))
```

The distributions are very close, so we proceed with adding the values to `df`.

```{r add mice values}
# save the mice output to df
df$Age <- mice_output$Age

sum(is.na(df$Age))
```

This concludes the imputing that we care about at this point. _(revisit later??)_

### __4.3 Feature Engineering Part Two__

Now, we create age dependent variables __Child__ and __Mother__.
```{r Mother and child graph, warning=FALSE, message=FALSE}
# First we'll look at the relationship between age & survival
ggplot(df[1:891,], aes(Age, fill = factor(Survived))) + 
  geom_histogram() + facet_grid(.~Sex)
```

```{r }
# create child variable
df$Child[df$Age < 18] <- 'Child'
df$Child[df$Age >= 18] <- 'Adult'

# table counts
table(df$Child, df$Survived)
```

So there is even odds of survival for children.  Now we will create a __Mother__ variable.

```{r Mother variable}
# create mother variable
df$Mother <- "Not Mother"
df$Mother[df$Sex == "female" & df$Parch > 0 & df$Age > 18 & df$Title != "Miss"] <- "Mother"

# table of counts
table(df$Mother, df$Survived)

# coerce to factor variables
df$Child <- factor(df$Child)
df$Mother <- factor(df$Mother)
```

We will check for missing data using the `md.pattern` function.

```{r md.pattern chk, warning=FALSE}
md.pattern(df)
```

We will proceed with modelling with this data.

# __5 Modelling__

### __5.1 Split data  into training and test sets__
```{r prepare data for modelling}
train <- df[1:891,]
test <- df[892:1309,]
```

### __5.2 A Decision Tree__
We will begin by looking at a decision tree for important clusters of data.

```{r decision tree}
# decision tree
D.tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + Disfamily_size + Child + Mother, data = train, method = "class")

# Fancy plot the decision tree
par(mfrow=c(1,1))
fancyRpartPlot(D.tree)
```

The decision tree indicates that __Title__ and __Pclass__ are the most important variables.  Next, we run a random forest.

### __5.3 Random Forest Model__

```{r random forest}
# set the seed 
set.seed(111)

# Apply the Random Forest Algorithm
rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + Disfamily_size + Child + Mother, data = train)

# show model error
plot(rf_model)
legend("topright", colnames(rf_model$err.rate), col = 1:3, fill = 1:3)
```

Our model appears to get a little worse at predicting survival.  That may be something to look at later for improvement.  We are improve at predicting death.

The most important variables are as follows.

```{r random forest variable importance}
# variable importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), Importance = round(importance[ ,'MeanDecreaseGini'],2))

# create a rank variable based on importance
rankImportance <- varImportance %>% mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), y = Importance, fill = Importance)) + geom_bar(stat='identity') + geom_text(aes(x = Variables, y = 0.5, label = Rank),hjust=0, vjust=0.55, size = 4, colour = 'red') + labs(x = 'Variables') + coord_flip()
```

Versus Megan Risdal'e's model, __Fare__ and __Sex__ have traded priority.  __Title__ is the most important which is means the feature engineering was successful. 

### __5.4 Prediction__

We create a prediction set to submit to Kaggle.
```{r predicton}
# create prediction from the test set
prediction <- predict(rf_model, test)

# create a data frame with PassengerId and Survived
solution <- data.frame(PassengerId = test[,1], Survived = prediction)

# Write solution to a csv file
write.csv(solution, file = "solution_rf1.csv", row.names = FALSE)

```
